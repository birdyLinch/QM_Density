{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import shutil\n",
    "import numpy\n",
    "# import pandas\n",
    "# import random\n",
    "# import ruamel.yaml as yaml\n",
    "import pickle\n",
    "# import torch\n",
    "# from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Literal, Union\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file structure:\n",
    "\n",
    "```bash\n",
    "agdb_null_test\n",
    "|---raw  # basic file dir\n",
    "|   |---[name].pkl\n",
    "|---extra  # basic file dir\n",
    "|   |---[name].attr1.pkl\n",
    "|   |---[name].attr2.pkl\n",
    "|---dataset_attributes.pkl  # basic file\n",
    "|---raw.tar  # tarfile of dir raw\n",
    "|---attr1.tar  # tarfile of extra/attr1\n",
    "|---attr2.tar  # tarfile of extra/attr2\n",
    "|---gen_data.ipynb  # for generating data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## null_test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './'\n",
    "assert os.path.basename(os.getcwd()) == 'agdb_null_test'  # check if in the right dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_attributes = {\n",
    "    \"xc\": \"m062x\",\n",
    "    \"basis\": \"def2tzvp\",\n",
    "    \"unit\": {\n",
    "        \"energy\": \"eV\",\n",
    "        \"length\": \"Bohr\",\n",
    "    },\n",
    "    \"doc\": \"\"\"This is a test dataset for building agdb modules\"\"\",\n",
    "}\n",
    "with open(os.path.join(root_dir, 'dataset_attributes.pkl'), 'wb') as f:\n",
    "    pickle.dump(dataset_attributes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_raw={'x': array([1, 3, 5, 6]), 'pos': array([[-1.63331094,  0.75480048,  0.9601171 ],\n",
      "       [-0.47974295,  1.01928488, -1.20201389],\n",
      "       [ 0.50422725,  2.32178196, -0.99859271],\n",
      "       [-0.15990044, -1.33673388, -1.677831  ]]), 'name': '001', 'etol': -0.08012218238361365},\n",
      "data_attr1={'attr1_1': array([-1.66051672, -0.45761608,  0.5147151 , -1.85360247, -0.0466721 ]), 'attr1_2': array([[-0.11453922, -0.09535394,  1.54399523],\n",
      "       [-0.63859771,  1.11285543, -0.56919963],\n",
      "       [-0.08672203,  1.29176386,  1.15865253],\n",
      "       [-0.44347449, -0.98350267,  1.64920487],\n",
      "       [-0.36067238, -0.17904819,  0.65143063]])},\n",
      "data_attr2={'attr2_1': array([0.98006123, 0.54315023, 3.39872933, 2.20801463, 0.76020792]), 'attr2_2': array([[-0.48417219,  0.37551309, -1.66056492],\n",
      "       [-0.10938012, -0.63680362, -0.25223974],\n",
      "       [ 0.80915984, -0.8739465 ,  1.09154243],\n",
      "       [-0.21891837, -0.02398193, -0.53424223],\n",
      "       [-0.52542046,  0.3889833 ,  1.09186296]])}\n",
      "data_raw={'x': array([3, 5, 7, 8]), 'pos': array([[ 0.75595399, -1.10338475, -0.75560776],\n",
      "       [ 2.2119807 , -0.1816105 , -1.81326387],\n",
      "       [-0.048929  , -0.86063727,  0.78493891],\n",
      "       [-1.51848869, -1.69797989, -0.02135102]]), 'name': '002', 'etol': 0.8420242398626263},\n",
      "data_attr1={'attr1_1': array([-0.4699485 ,  0.75963097,  0.48054192, -0.77609635,  1.55636033]), 'attr1_2': array([[ 1.21728737,  1.70203505, -0.47799159],\n",
      "       [ 0.57219672,  0.28930404,  0.55972364],\n",
      "       [-0.22765914,  1.22428147, -0.29596193],\n",
      "       [ 0.06783408,  0.41550847, -0.08876845],\n",
      "       [ 1.87531016,  0.90623395, -0.91561339]])},\n",
      "data_attr2={'attr2_1': array([-1.12582882,  0.35089098,  0.20522528,  0.60564817, -1.17820646]), 'attr2_2': array([[-1.0049089 ,  1.01362607,  0.37978771],\n",
      "       [ 0.73879593,  0.35701316, -1.1662109 ],\n",
      "       [ 0.05576144, -0.33333255,  0.17833744],\n",
      "       [ 0.29680304,  1.15420287,  0.48919476],\n",
      "       [-0.97027873,  0.55677412,  0.14618603]])}\n"
     ]
    }
   ],
   "source": [
    "rand_data_cnt = int(2)  # number of data to generate\n",
    "\n",
    "for i in range(rand_data_cnt):\n",
    "    natm = numpy.random.randint(4,7)\n",
    "    name = f'{i+1:03d}'\n",
    "    fname_raw = f\"{name}.pkl\"\n",
    "    fname_attr1 = fname_raw.replace(\".pkl\", \".attr1.pkl\")\n",
    "    fname_attr2 = fname_raw.replace(\".pkl\", \".attr2.pkl\")\n",
    "    data_raw = {\n",
    "        \"x\": numpy.random.randint(1,10,(natm,)),\n",
    "        \"pos\": numpy.random.randn(natm,3),\n",
    "        \"name\": name,\n",
    "        \"etol\": numpy.random.randn(1)[0],\n",
    "    }\n",
    "    data_attr1 = {\n",
    "        \"attr1_1\": numpy.random.randn(5,),\n",
    "        \"attr1_2\": numpy.random.randn(5,3),\n",
    "    }\n",
    "    data_attr2 = {\n",
    "        \"attr2_1\": numpy.random.randn(5,),\n",
    "        \"attr2_2\": numpy.random.randn(5,3),\n",
    "    }\n",
    "    with open(os.path.join(root_dir, \"raw\", fname_raw), 'wb') as f:\n",
    "        pickle.dump(data_raw, f)\n",
    "    with open(os.path.join(root_dir, \"extra\", fname_attr1), 'wb') as f:\n",
    "        pickle.dump(data_attr1, f)\n",
    "    with open(os.path.join(root_dir, \"extra\", fname_attr2), 'wb') as f:\n",
    "        pickle.dump(data_attr2, f)\n",
    "    print(f\"{data_raw=},\\n{data_attr1=},\\n{data_attr2=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme/louzekun/atom-graph-database/datatools/ceph_datasets/agdb_null_test\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/\n",
      "raw/001.pkl\n",
      "raw/002.pkl\n",
      "extra/001.attr1.pkl\n",
      "extra/002.attr1.pkl\n",
      "extra/001.attr2.pkl\n",
      "extra/002.attr2.pkl\n",
      "total 68K\n",
      "drwxrwxr-x. 5 louzekun louzekun 4.0K Jan 11 20:08 .\n",
      "drwxrwxr-x. 4 louzekun louzekun  130 Jan 11 19:02 ..\n",
      "-rw-rw-r--. 1 louzekun louzekun  314 Jan 11 19:45 anonymous_example.json\n",
      "-rw-rw-r--. 1 louzekun louzekun  316 Jan 11 19:43 anonymous.json\n",
      "-rw-rw-r--. 1 louzekun louzekun  10K Jan 11 20:08 attr1.tar\n",
      "-rw-rw-r--. 1 louzekun louzekun  10K Jan 11 20:08 attr2.tar\n",
      "-rw-rw-r--. 1 louzekun louzekun  146 Jan 11 19:38 dataset_attributes.pkl\n",
      "drwxrwxr-x. 2 louzekun louzekun  110 Jan 11 18:43 extra\n",
      "drwxrwxr-x. 2 louzekun louzekun    6 Jan 11 20:08 .ipynb_checkpoints\n",
      "-rw-rw-r--. 1 louzekun louzekun 9.6K Jan 11 19:46 proc_data.ipynb\n",
      "drwxrwxr-x. 2 louzekun louzekun   48 Jan 11 18:43 raw\n",
      "-rw-rw-r--. 1 louzekun louzekun  10K Jan 11 20:08 raw.tar\n",
      "-rw-rw-r--. 1 louzekun louzekun  713 Jan 11 20:08 README.md\n",
      "-rw-r--r--. 1 louzekun louzekun    0 Jan 11 19:46 syncRecord.txt\n"
     ]
    }
   ],
   "source": [
    "# tar files\n",
    "!tar -cvf raw.tar raw\n",
    "!tar -cvf attr1.tar extra/*.attr1.pkl\n",
    "!tar -cvf attr2.tar extra/*.attr2.pkl\n",
    "!ls -alh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023/01/11 20:08:20.544552 sensesync[127134] <INFO>: Found: 15, copied: 15, deleted: 0, failed: 0, Size: 43.193 KiB\n"
     ]
    }
   ],
   "source": [
    "# upload to ceph\n",
    "# NOTICE: the first line is dryrun!\n",
    "!sensesync --dryrun cp ./ s3://SJBRX22W1ND46X2GCRR9:DyWgIBe68vTOuBeLlpnwbvk1I6x6FnHsXkQXJkza@agdb_null_test.10.140.2.204:80/\n",
    "# !sensesync cp ./ s3://SJBRX22W1ND46X2GCRR9:DyWgIBe68vTOuBeLlpnwbvk1I6x6FnHsXkQXJkza@agdb_null_test.10.140.2.204:80/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (AccessDenied) when calling the PutBucketPolicy operation: Unknown\n"
     ]
    }
   ],
   "source": [
    "# maybe you have no access to change the policy, just contact admins of your ceph group\n",
    "# error might look like this: An error occurred (AccessDenied) when calling the PutBucketPolicy operation: Unknown\n",
    "!aws --endpoint-url=http://10.140.2.204:80 s3api put-bucket-policy --bucket agdb_null_test --policy file://./anonymous.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26f57fa64201eb68b7b35cc7b73c4bf19231ced7c340d267ba33da1e23b90450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
